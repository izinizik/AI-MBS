{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b38f23-d7c7-48f2-9f52-6fcedd735d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_curve\n",
    "from confidenceinterval import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, tnr_score\n",
    "from scipy.stats import ttest_ind\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bced30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FILE PATHS ===\n",
    "project_path = '/Users/labneuro2/Documents/lab/AI_MBS/AI_MBS'\n",
    "\n",
    "# === LOAD DATA ===\n",
    "df_features_reg = pd.read_excel(f'{project_path}/local_measures_combined_atlas.xlsx') \\\n",
    "    .sort_values(by='Subject').reset_index(drop=True)\n",
    "\n",
    "X_reg = df_features_reg[[col for col in df_features_reg.columns if col.split('_')[0] in ['fALFF','ALFF','ReHo']]].to_numpy()\n",
    "\n",
    "df_clinic = pd.read_excel(f'{project_path}/clinical_data.xlsx') \\\n",
    "    .sort_values(by='subject')\n",
    "\n",
    "clinic_cols = ['age_y','female_gender','type_2_diabetes','hypertension','hypothyroidism','depression',\n",
    "               'BMI_kgm2','waist_cm','hips_cm','waist_hip_ratio']\n",
    "binary_cols = ['female_gender','type_2_diabetes','hypertension','hypothyroidism','depression']\n",
    "X_clinic = df_clinic.loc[df_clinic['post_MBS'] == 0][clinic_cols].to_numpy()\n",
    "\n",
    "df_features_corr = pd.read_csv(f'{project_path}/FC_combined_atlas.csv') \\\n",
    "    .sort_values(by='Subject').reset_index(drop=True)\n",
    "\n",
    "X_corr = df_features_corr[[col for col in df_features_corr.columns if '_to_' in col]].to_numpy()\n",
    "\n",
    "\n",
    "# === COMBINE FEATURES ===\n",
    "X_fmri = np.hstack((X_corr, X_reg))\n",
    "X_all = np.hstack((X_fmri, X_clinic))\n",
    "\n",
    "X_reg_clinic = np.hstack((X_reg, X_clinic))\n",
    "X_corr_clinic = np.hstack((X_corr, X_clinic))\n",
    "\n",
    "# === LABELS: weight loss success (>50% overweight lost within a year) ===\n",
    "y = (df_clinic.loc[df_clinic['post_MBS'] == 1]['overweight_delta_proc_to_baseline_kg'] < -50).astype(int).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82a8823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureSelector selects top-k features based on t-test statistics between two classes\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k=10):\n",
    "        self.k = k\n",
    "        self.selected_features_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        t_values, p_values = ttest_ind(X[y == 0], X[y == 1], axis=0)\n",
    "        self.selected_features_ = np.argsort(np.abs(t_values))[-self.k:]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.selected_features_ is None:\n",
    "            raise ValueError(\"The FeatureSelector has not been fitted yet.\")\n",
    "        return X[:, self.selected_features_]\n",
    "\n",
    "# DynamicPreprocessor scales only continuous features, leaves binary features unchanged\n",
    "class DynamicPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Identify binary features\n",
    "        self.binary_mask_ = np.array([np.array_equal(np.unique(X[:, i]), [0, 1]) for i in range(X.shape[1])])\n",
    "        self.continuous_features_ = np.where(~self.binary_mask_)[0]\n",
    "        self.binary_features_ = np.where(self.binary_mask_)[0]\n",
    "        # Fit scaler only on continuous features\n",
    "        self.scaler_ = StandardScaler().fit(X[:, self.continuous_features_])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        # Scale continuous features, leave binary features unchanged\n",
    "        X_transformed[:, self.continuous_features_] = self.scaler_.transform(X[:, self.continuous_features_])\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd569621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fold(train_idx, test_idx, X, y, option):\n",
    "    # Suppress warnings for cleaner output\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "    # Extract parameters for this fold\n",
    "    param_grid = option['param_grid'] \n",
    "    cv = option['cv'] \n",
    "    clf = option['classifier']\n",
    "\n",
    "    # Split data into training and test sets for this fold\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Build pipeline: feature selection, scaling, then classification\n",
    "    pipeline = Pipeline([\n",
    "        ('selector', FeatureSelector()),\n",
    "        ('scaler', DynamicPreprocessor()),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    \n",
    "    # Perform grid search cross-validation to find best hyperparameters\n",
    "    best_pipeline = GridSearchCV(pipeline, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Retrieve best feature selector and scaler from the pipeline\n",
    "    best_selector = best_pipeline.best_estimator_['selector']\n",
    "    best_scaler = best_pipeline.best_estimator_['scaler']\n",
    "    \n",
    "    # Apply feature selection to train and test sets\n",
    "    X_train_selected = best_selector.transform(X_train)\n",
    "    X_test_selected = best_selector.transform(X_test)\n",
    "\n",
    "    # Scale the selected features\n",
    "    X_train_scaled = best_scaler.transform(X_train_selected)\n",
    "    X_test_scaled = best_scaler.transform(X_test_selected)\n",
    "\n",
    "    # Get the best classifier and predict probabilities for the test sample\n",
    "    best_model = best_pipeline.best_estimator_['classifier']\n",
    "    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # Return true label and predicted probability for this fold\n",
    "    return y_test[0], y_pred_proba[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d6499b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loo_with_options(X, y, options):\n",
    "    # This function performs Leave-One-Out cross-validation for each option in options.\n",
    "    # For each option, it collects predictions for all folds, computes performance metrics,\n",
    "    # and prints summary statistics.\n",
    "    results = []\n",
    "\n",
    "    for option in options:\n",
    "        # Leave-One-Out cross-validation\n",
    "        loo = LeaveOneOut()\n",
    "        # Parallelize the processing of each fold\n",
    "        fold_results = Parallel(n_jobs=-1)(\n",
    "            delayed(process_fold)(train_idx, test_idx, X, y, option) for train_idx, test_idx in loo.split(X, y)\n",
    "        )\n",
    "\n",
    "        # Collect true labels and predicted probabilities from all folds\n",
    "        y_true_all = np.array([res[0] for res in fold_results])\n",
    "        y_pred_proba_all = np.array([res[1] for res in fold_results])\n",
    "        y_pred = (y_pred_proba_all >= .5).astype(int)\n",
    "        \n",
    "        # Calculate metrics and their confidence intervals\n",
    "        auc, auc_ci = roc_auc_score(y_true_all, y_pred_proba_all, confidence_level=0.95)\n",
    "        accuracy, accuracy_ci = accuracy_score(y_true_all, y_pred, confidence_level=0.95)\n",
    "        sensitivity, sensitivity_ci = recall_score(y_true_all, y_pred, confidence_level=0.95)\n",
    "        specificity, specificity_ci = tnr_score(y_true_all, y_pred, confidence_level=0.95)\n",
    "        precision, precision_ci = precision_score(y_true_all, y_pred, confidence_level=0.95)\n",
    "        f1, f1_ci = f1_score(y_true_all, y_pred, confidence_level=0.95)\n",
    "\n",
    "        # Store results for this option\n",
    "        results.append({\n",
    "            'model': option['classifier'],\n",
    "            'param_grid': option['param_grid'],\n",
    "            'cv': option['cv'],\n",
    "            'accuracy': round(accuracy, 4),\n",
    "            'auc': round(auc, 4),\n",
    "            'sensitivity': round(sensitivity, 4),\n",
    "            'specificity': round(specificity, 4),\n",
    "            'precision': round(precision, 4),\n",
    "            'f1': round(f1, 4),\n",
    "            'accuracy_ci': [round(c, 4) for c in accuracy_ci],\n",
    "            'auc_ci': [round(c, 4) for c in auc_ci],\n",
    "            'sensitivity_ci': [round(c, 4) for c in sensitivity_ci],\n",
    "            'specificity_ci': [round(c, 4) for c in specificity_ci],\n",
    "            'precision_ci': [round(c, 4) for c in precision_ci],\n",
    "            'f1_ci': [round(c, 4) for c in f1_ci],\n",
    "            'y_pred_proba': y_pred_proba_all\n",
    "        })\n",
    "        # Print summary for this option\n",
    "        print(f\"Option: {results[-1]['model']}, params {results[-1]['param_grid']}\")\n",
    "        print(f\" ACC: {results[-1]['accuracy']:.3f}, AUC: {results[-1]['auc']:.3f}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45673c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from sklearn/xgboost class names to human-readable model names\n",
    "MODEL_NAME_MAPPING = {\n",
    "    \"SVC\": \"Support Vector Machine\",\n",
    "    \"LogisticRegression\": \"Logistic Regression\",\n",
    "    \"RandomForest\": \"Random Forest\",\n",
    "    \"XGBClassifier\": \"Extreme Gradient Boosting\",\n",
    "    \"MLPClassifier\": \"Multi-Layer Perceptron\",\n",
    "}\n",
    "\n",
    "# Function to intelligently recognize model name from its repr()\n",
    "def map_model_name(classifier):\n",
    "    classifier_repr = repr(classifier)  # Get string representation, e.g., \"SVC(probability=True, random_state=42)\"\n",
    "    for key, name in MODEL_NAME_MAPPING.items():\n",
    "        if key in classifier_repr:  # If key is found in repr(), return mapped name\n",
    "            return name\n",
    "    return \"Unknown Model\"  # Return default if no match\n",
    "\n",
    "# Format hyperparameters into separate lines for readability\n",
    "def format_hyperparameters(param_grid):\n",
    "    formatted_params = []\n",
    "    for key, value in param_grid.items():\n",
    "        param_name = key.replace(\"classifier__\", \"\").replace(\"_\", \" \").capitalize()\n",
    "        formatted_params.append(f\"{param_name}: {', '.join(map(str, value)) if isinstance(value, list) else value}\")\n",
    "    return \"\\n\".join(formatted_params)\n",
    "\n",
    "# Merge metric value with its confidence interval for reporting\n",
    "def merge_metric_ci(df, metric):\n",
    "    df[metric] = df[metric].astype(str) + \"\\n(\" + df[metric + \"_ci\"].astype(str) + \")\"\n",
    "    df.drop(columns=[metric + \"_ci\"], inplace=True)\n",
    "\n",
    "# Format metric column names for Word-style output\n",
    "def format_metric_column_name(metric):\n",
    "    return f\"{metric.capitalize()}\\n(95% CI)\"\n",
    "\n",
    "# List of metrics to process\n",
    "metrics = [\"accuracy\", \"auc\", \"sensitivity\", \"specificity\", \"precision\", \"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6885693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifiers_and_param_sets(random_state):\n",
    "    # Returns a dictionary of classifiers and their hyperparameter grids for model selection\n",
    "    classifiers = {\n",
    "        'svm': {\n",
    "            'classifier': SVC(probability=True, random_state=random_state),\n",
    "            'param_sets': [\n",
    "                {'classifier__C': [0.01, 0.1, 1], 'classifier__kernel': ['linear']},\n",
    "                {'classifier__C': [0.01, 0.1, 1], 'classifier__kernel': ['rbf']},\n",
    "                {'classifier__kernel': ['rbf'], 'classifier__gamma': ['scale', 'auto', 0.01, 0.1]},\n",
    "                {'classifier__kernel': ['rbf'], 'classifier__gamma': ['scale', 'auto', 0.001, 0.01]},\n",
    "                {'classifier__C': [0.01, 0.1, 1], 'classifier__kernel': ['rbf'], 'classifier__gamma': ['scale', 'auto', 0.01, 0.1]},\n",
    "                {'classifier__C': [0.01, 0.1, 1], 'classifier__kernel': ['rbf'], 'classifier__gamma': ['scale', 'auto', 0.001, 0.01]},\n",
    "            ]\n",
    "        },\n",
    "        'logistic_regression': {\n",
    "            'classifier': LogisticRegression(random_state=random_state),\n",
    "            'param_sets': [\n",
    "                {'classifier__penalty': ['l2'], 'classifier__C': [0.1, 1, 10]},\n",
    "                {'classifier__penalty': ['l1'], 'classifier__C': [0.1, 1, 10], 'classifier__solver': ['liblinear']},\n",
    "                {'classifier__penalty': ['elasticnet'], 'classifier__C': [0.1, 1, 10], 'classifier__solver': ['saga'], 'classifier__l1_ratio': [0.1, 0.5, 0.9]},\n",
    "                {'classifier__penalty': ['l2'], 'classifier__C': [0.01, 0.1, 1]},\n",
    "                {'classifier__penalty': ['l1'], 'classifier__C': [0.01, 0.1, 1], 'classifier__solver': ['liblinear']},\n",
    "                {'classifier__penalty': ['elasticnet'], 'classifier__C': [0.01, 0.1, 1], 'classifier__solver': ['saga'], 'classifier__l1_ratio': [0.1, 0.5, 0.9]},\n",
    "            ]\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'classifier': RandomForestClassifier(random_state=random_state),\n",
    "            'param_sets': [\n",
    "                {'classifier__n_estimators': [50, 100, 200], 'classifier__max_depth': [5, 10, None]},\n",
    "                {'classifier__n_estimators': [10, 50, 100], 'classifier__max_depth': [5, 10, None]},\n",
    "                {'classifier__n_estimators': [50, 100, 200], 'classifier__max_depth': [5, 10, None], 'classifier__min_samples_leaf': [1, 2, 4]},\n",
    "                {'classifier__n_estimators': [10, 50, 100], 'classifier__max_depth': [5, 10, None], 'classifier__min_samples_leaf': [1, 2, 4]},\n",
    "            ]\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'classifier': XGBClassifier(eval_metric='logloss', random_state=random_state),\n",
    "            'param_sets': [\n",
    "                {'classifier__learning_rate': [0.01, 0.1, 0.3]},\n",
    "                {'classifier__booster': ['gbtree', 'dart']},\n",
    "                {'classifier__learning_rate': [0.01, 0.1, 0.3], 'classifier__booster': ['gbtree', 'dart']},\n",
    "            ]\n",
    "        },\n",
    "        'mlp': {\n",
    "            'classifier': MLPClassifier(random_state=random_state),\n",
    "            'param_sets': [\n",
    "                {'classifier__hidden_layer_sizes': [(50,), (100,)]},\n",
    "                {'classifier__hidden_layer_sizes': [(50,), (100,), (100, 50)]},\n",
    "                {'classifier__hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 50, 50)]},\n",
    "                {'classifier__hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 50, 50), (100, 100, 50, 50)]},\n",
    "                {'classifier__hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 50, 50), (100, 100, 50, 50), (100, 100, 100, 50, 50)]},\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    return classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c07b783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates a list of option dictionaries for model selection.\n",
    "# Each option specifies a classifier, its hyperparameter grid, and feature selection range.\n",
    "def generate_options_with_classifiers():\n",
    "    t_stat_ranges = [np.arange(10, 21, 1)]  # Range of top-k features to select (from 10 to 20)\n",
    "\n",
    "    options = []\n",
    "    classifiers = get_classifiers_and_param_sets(random_state)  # Get classifiers and their parameter sets\n",
    "    for name, classifier_info in classifiers.items():\n",
    "        for param_set in classifier_info['param_sets']:\n",
    "            for t_range in t_stat_ranges:\n",
    "                options.append({\n",
    "                        'param_grid': {\n",
    "                            **param_set,\n",
    "                            'selector__k': list(t_range)  # Add feature selector parameter\n",
    "                        },\n",
    "                        'cv': 10,\n",
    "                        'classifier': classifier_info['classifier'],\n",
    "                        'name': f\"{name}_t_stat\"\n",
    "                })\n",
    "    return options\n",
    "\n",
    "options = generate_options_with_classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c9ecf-93e6-495c-ad91-f1a3ad24cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [X_clinic, X_corr, X_reg, X_all, X_fmri, X_reg_clinic, X_corr_clinic]\n",
    "inputs_names = ['clinic', 'corr', 'reg', 'all', 'fmri', 'reg_clinic', 'FC_clinic']\n",
    "\n",
    "# Process results separately for each input dataset\n",
    "for X, Xname in zip(inputs, inputs_names):\n",
    "    results = run_loo_with_options(X, y, options)\n",
    "    df_res = pd.DataFrame(results)\n",
    "\n",
    "    # Intelligent mapping of model names based on `repr()`\n",
    "    df_res[\"Model\"] = df_res[\"model\"].apply(map_model_name)\n",
    "\n",
    "    # Formatting hyperparameters\n",
    "    df_res[\"Hyperparameter Grid\"] = df_res[\"param_grid\"].apply(format_hyperparameters)\n",
    "\n",
    "    # Formatting metrics\n",
    "    for metric in metrics:\n",
    "        merge_metric_ci(df_res, metric)\n",
    "\n",
    "    # Change metric column names to Word-style format\n",
    "    column_mapping = {metric: format_metric_column_name(metric) for metric in metrics}\n",
    "    df_res.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "    # Reorder columns to match Word-style output\n",
    "    df_res = df_res[[\"Model\", \"Hyperparameter Grid\"] + list(column_mapping.values())]\n",
    "\n",
    "    # Save to separate files for each Xname\n",
    "    output_file = f\"{project_path}/formatted_models_result_{Xname}.xlsx\"\n",
    "    df_res.to_excel(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e3db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
